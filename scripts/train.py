"""è®­ç»ƒæ¨¡å—"""


import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import os
from tqdm import tqdm
from models.model import create_model
from utils.data_utils import create_data_loaders
import numpy as np


def train_model(
        model,
        train_loader,
        val_loader,
        num_epochs=15,
        learning_rate=0.0001,
        weight_decay=1e-5,
        device="cuda" if torch.cuda.is_available() else "cpu",
        save_path="../results/models",
        model_name="resnet50"
):
    """
    è®­ç»ƒç”²çŠ¶è…ºç»“èŠ‚åˆ†ç±»æ¨¡å‹ï¼ŒåŒ…å«å¢å¼ºçš„è®­ç»ƒç­–ç•¥å’Œæ—©åœæœºåˆ¶
    """
    os.makedirs(save_path, exist_ok=True)

    # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼ˆè®¡ç®—ç±»åˆ«æƒé‡ï¼‰
    if hasattr(train_loader.dataset, 'class_distribution'):
        class_counts = np.array([train_loader.dataset.class_distribution.get(0, 1),
                                 train_loader.dataset.class_distribution.get(1, 1)])
        class_weights = 1.0 / class_counts
        class_weights = class_weights / np.sum(class_weights) * 2  # å½’ä¸€åŒ–
        weight_tensor = torch.FloatTensor(class_weights).to(device)
        criterion = nn.CrossEntropyLoss(weight=weight_tensor)
        print(f"ä½¿ç”¨ç±»åˆ«æƒé‡: {class_weights}")
    else:
        criterion = nn.CrossEntropyLoss()

    # ä¼˜åŒ–å™¨ä¸å­¦ä¹ ç‡è°ƒåº¦
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=2,
        verbose=True, min_lr=1e-6
    )

    model = model.to(device)
    print(f"ä½¿ç”¨è®¾å¤‡: {device}, æ¨¡å‹: {model_name}")

    best_val_acc = 0.0
    best_model_path = os.path.join(save_path, f"best_{model_name}.pth")
    early_stopping_counter = 0
    early_stopping_limit = 10  # æ—©åœé˜ˆå€¼

    history = {"train_loss": [], "train_acc": [], "val_loss": [], "val_acc": []}

    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch + 1}/{num_epochs}")
        print("-" * 50)

        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0

        with tqdm(train_loader, desc="è®­ç»ƒä¸­") as pbar:
            for inputs, labels in pbar:
                inputs, labels = inputs.to(device), labels.to(device)

                optimizer.zero_grad()
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()

                train_loss += loss.item() * inputs.size(0)
                _, preds = torch.max(outputs, 1)
                train_total += labels.size(0)
                train_correct += (preds == labels).sum().item()

                pbar.set_postfix(loss=loss.item(), acc=100.0 * train_correct / train_total)

        epoch_train_loss = train_loss / len(train_loader.dataset)
        epoch_train_acc = 100.0 * train_correct / train_total
        history["train_loss"].append(epoch_train_loss)
        history["train_acc"].append(epoch_train_acc)
        print(f"è®­ç»ƒæŸå¤±: {epoch_train_loss:.4f}, å‡†ç¡®ç‡: {epoch_train_acc:.2f}%")

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0

        with torch.no_grad(), tqdm(val_loader, desc="éªŒè¯ä¸­") as pbar:
            for inputs, labels in pbar:
                inputs, labels = inputs.to(device), labels.to(device)

                outputs = model(inputs)
                loss = criterion(outputs, labels)

                val_loss += loss.item() * inputs.size(0)
                _, preds = torch.max(outputs, 1)
                val_total += labels.size(0)
                val_correct += (preds == labels).sum().item()

                pbar.set_postfix(loss=loss.item(), acc=100.0 * val_correct / val_total)

        epoch_val_loss = val_loss / len(val_loader.dataset)
        epoch_val_acc = 100.0 * val_correct / val_total
        history["val_loss"].append(epoch_val_loss)
        history["val_acc"].append(epoch_val_acc)
        print(f"éªŒè¯æŸå¤±: {epoch_val_loss:.4f}, å‡†ç¡®ç‡: {epoch_val_acc:.2f}%")

        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step(epoch_val_acc)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if epoch_val_acc > best_val_acc:
            best_val_acc = epoch_val_acc
            torch.save({
                "model_state_dict": model.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "epoch": epoch,
                "accuracy": best_val_acc,
                "history": history,
                "model_name": model_name
            }, best_model_path)
            print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
            early_stopping_counter = 0
        else:
            early_stopping_counter += 1
            print(f"ğŸ”„ æœªæ”¹è¿›ï¼Œæ—©åœè®¡æ•°å™¨: {early_stopping_counter}/{early_stopping_limit}")

        # æ—©åœæœºåˆ¶
        if early_stopping_counter >= early_stopping_limit:
            print(f"âš ï¸ è¾¾åˆ°æ—©åœæ¡ä»¶ï¼Œæå‰ç»ˆæ­¢è®­ç»ƒ")
            break

    print(f"\nè®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
    return model, history


if __name__ == "__main__":
    # æ•°æ®è·¯å¾„ï¼ˆæ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰
    train_csv = "../data/Thyroid_nodule_Dataset/label4train.csv"
    val_csv = "../data/Thyroid_nodule_Dataset/label4test.csv"
    train_dir = "../data/Thyroid_nodule_Dataset/train-image"
    val_dir = "../data/Thyroid_nodule_Dataset/test-image"

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆå¢åŠ batch_sizeå’Œnum_workersï¼‰
    train_loader, val_loader = create_data_loaders(
        train_csv=train_csv,
        val_csv=val_csv,
        train_dir=train_dir,
        val_dir=val_dir,
        batch_size=32,
        image_size=256,  # å¢å¤§å›¾åƒå°ºå¯¸
        num_workers=4  # æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´
    )

    # åˆ›å»ºæ¨¡å‹ï¼ˆä½¿ç”¨æ›´æ·±çš„ResNet50ï¼‰
    model = create_model(num_classes=2, model_name="resnet50")
    print(model)

    # è®­ç»ƒæ¨¡å‹ï¼ˆå¢åŠ è®­ç»ƒè½®æ•°ï¼Œé™ä½åˆå§‹å­¦ä¹ ç‡ï¼‰
    trained_model, history = train_model(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=100,
        learning_rate=0.0001
    )