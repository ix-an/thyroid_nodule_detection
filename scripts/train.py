"""è®­ç»ƒæ¨¡å—"""


import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import os
from tqdm import tqdm
from models.model import create_model
from utils.data_utils import create_data_loaders
import numpy as np


def train_model(
        model,
        train_loader,
        val_loader,
        num_epochs=15,
        learning_rate=0.0001,
        weight_decay=1e-5,
        device="cuda" if torch.cuda.is_available() else "cpu",
        save_path="../results/models",
        model_name="resnet50"
):
    """
    è®­ç»ƒç”²çŠ¶è…ºç»“èŠ‚åˆ†ç±»æ¨¡å‹ï¼ŒåŒ…å«å¢å¼ºçš„è®­ç»ƒç­–ç•¥å’Œæ—©åœæœºåˆ¶
    """
    # åˆ›å»ºæ¨¡å‹ä¿å­˜è·¯å¾„
    os.makedirs(save_path, exist_ok=True)

    # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼ˆè®¡ç®—ç±»åˆ«æƒé‡ï¼‰
    if hasattr(train_loader.dataset, 'class_distribution'):
        # è·å–è®­ç»ƒé›†çš„ç±»åˆ«åˆ†å¸ƒ
        class_counts = np.array([train_loader.dataset.class_distribution.get(0, 1),
                                 train_loader.dataset.class_distribution.get(1, 1)])
        # è®¡ç®—ç±»åˆ«æƒé‡ï¼ˆä½¿ç”¨ç±»åˆ«é¢‘ç‡çš„å€’æ•°ï¼‰
        class_weights = 1.0 / class_counts
        class_weights = class_weights / np.sum(class_weights) * 2  # å½’ä¸€åŒ–
        weight_tensor = torch.FloatTensor(class_weights).to(device)
        # ä½¿ç”¨å¸¦æƒé‡çš„äº¤å‰ç†µæŸå¤±å‡½æ•°
        criterion = nn.CrossEntropyLoss(weight=weight_tensor)
        print(f"ä½¿ç”¨ç±»åˆ«æƒé‡: {class_weights}")
    else:
        # è‹¥æ²¡æœ‰ç±»åˆ«åˆ†å¸ƒä¿¡æ¯ï¼Œåˆ™ä½¿ç”¨æ™®é€šçš„äº¤å‰ç†µæŸå¤±å‡½æ•°
        criterion = nn.CrossEntropyLoss()

    # ä¼˜åŒ–å™¨ä¸å­¦ä¹ ç‡è°ƒåº¦
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼šæ ¹æ®éªŒè¯é›†æ€§èƒ½åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡
    # å½“éªŒè¯é›†çš„æ€§èƒ½åœ¨ä¸€æ®µæ—¶é—´å†…æ²¡æœ‰æå‡æ—¶ï¼Œå®ƒä¼šé™ä½å­¦ä¹ ç‡ï¼Œä»¥å¸®åŠ©æ¨¡å‹è·³å‡ºå±€éƒ¨æœ€ä¼˜è§£ï¼Œç»§ç»­å¯»æ‰¾æ›´ä¼˜çš„å‚æ•°ã€‚
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=2,
        verbose=True, min_lr=1e-6
    )

    # å°†æ¨¡å‹ç§»è‡³æŒ‡å®šè®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰
    model = model.to(device)
    print(f"ä½¿ç”¨è®¾å¤‡: {device}, æ¨¡å‹: {model_name}")

    best_val_acc = 0.0
    best_model_path = os.path.join(save_path, f"best_{model_name}.pth")
    early_stopping_counter = 0
    early_stopping_limit = 10    # æ—©åœé˜ˆå€¼

    history = {"train_loss": [], "train_acc": [], "val_loss": [], "val_acc": []}

    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch + 1}/{num_epochs}")
        print("-" * 50)

        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0

        # ä½¿ç”¨tqdmæ˜¾ç¤ºè®­ç»ƒè¿›åº¦æ¡
        with tqdm(train_loader, desc="è®­ç»ƒä¸­") as pbar:
            for inputs, labels in pbar:
                # å°†è¾“å…¥å’Œæ ‡ç­¾ç§»è‡³æŒ‡å®šè®¾å¤‡
                inputs, labels = inputs.to(device), labels.to(device)

                # æ¢¯åº¦æ¸…é›¶
                optimizer.zero_grad()
                # å‰å‘ä¼ æ’­
                outputs = model(inputs)
                # è®¡ç®—æŸå¤±
                loss = criterion(outputs, labels)
                # åå‘ä¼ æ’­
                loss.backward()
                # æ›´æ–°å‚æ•°
                optimizer.step()

                train_loss += loss.item() * inputs.size(0)
                _, preds = torch.max(outputs, 1)
                train_total += labels.size(0)
                train_correct += (preds == labels).sum().item()

                pbar.set_postfix(loss=loss.item(), acc=100.0 * train_correct / train_total)

        epoch_train_loss = train_loss / len(train_loader.dataset)
        epoch_train_acc = 100.0 * train_correct / train_total
        history["train_loss"].append(epoch_train_loss)
        history["train_acc"].append(epoch_train_acc)
        print(f"è®­ç»ƒæŸå¤±: {epoch_train_loss:.4f}, å‡†ç¡®ç‡: {epoch_train_acc:.2f}%")

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0

        # ä½¿ç”¨tqdmæ˜¾ç¤ºéªŒè¯è¿›åº¦æ¡
        with torch.no_grad(), tqdm(val_loader, desc="éªŒè¯ä¸­") as pbar:
            for inputs, labels in pbar:
                # å°†è¾“å…¥å’Œæ ‡ç­¾ç§»è‡³æŒ‡å®šè®¾å¤‡
                inputs, labels = inputs.to(device), labels.to(device)

                # å‰å‘ä¼ æ’­
                outputs = model(inputs)
                # è®¡ç®—æŸå¤±
                loss = criterion(outputs, labels)

                # ç´¯åŠ å½“å‰æ‰¹æ¬¡çš„æ€»æŸå¤±
                val_loss += loss.item() * inputs.size(0)
                # è·å–æ¨¡å‹é¢„æµ‹çš„ç±»åˆ«ï¼ˆæ¦‚ç‡æœ€é«˜çš„ç±»åˆ«ç´¢å¼•ï¼‰
                _, preds = torch.max(outputs, 1)
                # ç´¯åŠ å½“å‰æ‰¹æ¬¡çš„æ ·æœ¬æ€»æ•°
                val_total += labels.size(0)
                # è®¡ç®—å½“å‰æ‰¹æ¬¡é¢„æµ‹æ­£ç¡®çš„æ ·æœ¬æ•°ï¼Œç„¶åç´¯åŠ åˆ°æ€»æ­£ç¡®æ•°ä¸­
                val_correct += (preds == labels).sum().item()
                # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤ºå½“å‰æ‰¹æ¬¡çš„æŸå¤±å’Œç´¯è®¡å‡†ç¡®ç‡
                pbar.set_postfix(loss=loss.item(), acc=100.0 * val_correct / val_total)

        # è®¡ç®—æ•´ä¸ªéªŒè¯é›†çš„å¹³å‡æŸå¤±
        epoch_val_loss = val_loss / len(val_loader.dataset)
        # è®¡ç®—æ•´ä¸ªéªŒè¯é›†çš„å‡†ç¡®ç‡
        epoch_val_acc = 100.0 * val_correct / val_total
        # è®°å½•æœ¬è½®è®­ç»ƒçš„éªŒè¯æŒ‡æ ‡
        history["val_loss"].append(epoch_val_loss)
        history["val_acc"].append(epoch_val_acc)
        # æ‰“å°æœ¬è½®éªŒè¯ç»“æœ
        print(f"éªŒè¯æŸå¤±: {epoch_val_loss:.4f}, å‡†ç¡®ç‡: {epoch_val_acc:.2f}%")

        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step(epoch_val_acc)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if epoch_val_acc > best_val_acc:
            best_val_acc = epoch_val_acc
            torch.save({
                "model_state_dict": model.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "epoch": epoch,
                "accuracy": best_val_acc,
                "history": history,
                "model_name": model_name
            }, best_model_path)
            print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
            early_stopping_counter = 0
        else:
            early_stopping_counter += 1
            print(f"ğŸ”„ æœªæ”¹è¿›ï¼Œæ—©åœè®¡æ•°å™¨: {early_stopping_counter}/{early_stopping_limit}")

        # æ—©åœæœºåˆ¶
        if early_stopping_counter >= early_stopping_limit:
            print(f"âš ï¸ è¾¾åˆ°æ—©åœæ¡ä»¶ï¼Œæå‰ç»ˆæ­¢è®­ç»ƒ")
            break

    print(f"\nè®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
    return model, history


if __name__ == "__main__":
    # æ•°æ®è·¯å¾„
    train_csv = "../data/Thyroid_nodule_Dataset/label4train.csv"
    val_csv = "../data/Thyroid_nodule_Dataset/label4test.csv"
    train_dir = "../data/Thyroid_nodule_Dataset/train-image"
    val_dir = "../data/Thyroid_nodule_Dataset/test-image"

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆå¢åŠ batch_sizeå’Œnum_workersï¼‰
    train_loader, val_loader = create_data_loaders(
        train_csv=train_csv,
        val_csv=val_csv,
        train_dir=train_dir,
        val_dir=val_dir,
        batch_size=32,
        image_size=256,  # å¢å¤§å›¾åƒå°ºå¯¸
        num_workers=4  # æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´
    )

    # åˆ›å»ºæ¨¡å‹ï¼ˆä½¿ç”¨æ›´æ·±çš„ResNet50ï¼‰
    model = create_model(num_classes=2, model_name="resnet50")
    print(model)

    # è®­ç»ƒæ¨¡å‹ï¼ˆå¢åŠ è®­ç»ƒè½®æ•°ï¼Œé™ä½åˆå§‹å­¦ä¹ ç‡ï¼‰
    trained_model, history = train_model(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=100,
        learning_rate=0.0001
    )